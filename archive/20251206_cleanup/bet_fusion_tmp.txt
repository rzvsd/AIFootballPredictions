"""
bet_fusion.py â€” Fusion engine and CLI

Exports generate_predictions(config) to compute 1X2 probabilities by fusing
champion XGB (home/away expected goals) with Dixon-Coles. Also includes a
simple CLI that fetches odds and logs bets via BankrollManager.
"""
import os
import time
from pathlib import Path
from typing import Optional, Dict, Any
import yaml
import csv
import json

import pandas as pd
import requests
from requests.exceptions import RequestException
import joblib
from xgboost import XGBRegressor
import numpy as np
from scipy.stats import poisson
import math
import difflib

# Import our custom modules
from scripts import bookmaker_api
import xgb_trainer
import config
import feature_store
import team_registry

# --- CONFIGURATION LOADER ---
CONFIG_PATH = Path(__file__).parent / "bot_config.yaml"

def load_config(path: Path = CONFIG_PATH) -> Dict[str, Any]:
    """Load bot configuration from a YAML file."""
    with open(path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f) or {}
    # Apply environment overrides if necessary (good for servers)
    config["stake_size"] = float(os.getenv("BOT_STAKE_SIZE", config.get("stake_size", 10.0)))
    config["probability_threshold"] = float(os.getenv("BOT_PROB_THRESHOLD", config.get("probability_threshold", 0.6)))
    config["edge_requirement"] = float(os.getenv("BOT_EDGE_REQUIREMENT", config.get("edge_requirement", 0.05)))
    # Logging switch
    lb = os.getenv("BOT_LOG_BETS")
    if lb is not None:
        config["log_bets"] = str(lb).strip().lower() in ("1","true","yes","on")
    else:
        config["log_bets"] = bool(config.get("log_bets", True))
    # Stake policy
    sp = os.getenv("BOT_STAKE_POLICY") or config.get("stake_policy", "flat")
    config["stake_policy"] = str(sp).strip().lower()
    try:
        config["kelly_fraction"] = float(os.getenv("BOT_KELLY_FRACTION", config.get("kelly_fraction", 0.25)))
    except Exception:
        config["kelly_fraction"] = 0.25
    # Min odds filter (drop very low prices)
    try:
        config["min_odds"] = float(os.getenv("BOT_MIN_ODDS", config.get("min_odds", 1.6)))
    except Exception:
        config["min_odds"] = 1.6

    # League override: allow environment to set the active league
    try:
        league_env = os.getenv("BOT_LEAGUE")
        if league_env:
            config["league"] = str(league_env).strip().upper()
        else:
            # normalize whatever is in YAML
            config["league"] = str(config.get("league", "E0")).strip().upper()
    except Exception:
        config["league"] = str(config.get("league", "E0")).strip().upper()
    return config

# --- BANKROLL & LOGGING (Your new class) ---
class BankrollManager:
    """Simple bankroll tracker and logger for bets."""
    def __init__(self, bankroll_file: str = "data/bankroll.json", log_file: str = "data/bets_log.csv"):
        self.bankroll_path = Path(bankroll_file)
        self.log_path = Path(log_file)
        self.bankroll = self._load_bankroll()
        print(f"Bankroll initialized at: {self.bankroll:.2f}")

    def _load_bankroll(self) -> float:
        self.bankroll_path.parent.mkdir(parents=True, exist_ok=True)
        if self.bankroll_path.exists():
            with open(self.bankroll_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return float(data.get("bankroll", 1000.0)) # Default to 1000 if not set
        return 1000.0

    def _save_bankroll(self) -> None:
        with open(self.bankroll_path, "w", encoding="utf-8") as f:
            json.dump({"bankroll": self.bankroll}, f)

    def log_bet(self, date: str, league: str, home: str, away: str, market: str, odds: float, stake: float, prob: float, ev: float):
        """Deduct stake and log the placed bet."""
        if stake > self.bankroll:
            print(f"Warning: Insufficient bankroll ({self.bankroll:.2f}) for stake ({stake:.2f}). Skipping bet.")
            return
        self.bankroll -= stake
        self._save_bankroll()
        self._append_log(date, league, home, away, market, odds, stake, prob, ev)
        print(f"Bet Logged. New bankroll: {self.bankroll:.2f}")

    def _append_log(self, date, league, home, away, market, odds, stake, prob, ev):
        self.log_path.parent.mkdir(parents=True, exist_ok=True)
        file_exists = self.log_path.exists()
        with open(self.log_path, "a", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(["date", "league", "home_team", "away_team", "market", "odds", "stake", "model_prob", "expected_value"])
            writer.writerow([date, league, home, away, market, odds, stake, f"{prob:.4f}", f"{ev:.4f}"])

# --- Helper functions used by the fusion engine ---
def _score_matrix(mu_h: float, mu_a: float, max_goals: int = 10, trim_epsilon: float = 0.0) -> np.ndarray:
    hg = np.arange(0, max_goals + 1)
    ag = np.arange(0, max_goals + 1)
    ph = np.exp(-mu_h) * (mu_h ** hg) / np.array([math.factorial(int(i)) for i in hg])
    pa = np.exp(-mu_a) * (mu_a ** ag) / np.array([math.factorial(int(i)) for i in ag])
    P = np.outer(ph, pa)
    s = P.sum()
    if s > 0:
        P /= s
    if trim_epsilon and trim_epsilon > 0:
        P = np.where(P < trim_epsilon, 0.0, P)
        s2 = P.sum()
        if s2 > 0:
            P /= s2
    return P
def _score_matrix_negbin(mu_h: float, mu_a: float, k_h: float, k_a: float, max_goals: int = 10) -> np.ndarray:
    """Negative Binomial goal matrix with dispersion k (r)."""
    def nb_pmf_vec(mu: float, k: float, n: int) -> np.ndarray:
        mu = max(1e-9, float(mu)); k = max(1e-6, float(k))
        p = k / (k + mu)
        xs = np.arange(0, n+1, dtype=float)
        from math import lgamma
        logC = [lgamma(x + k) - lgamma(k) - lgamma(x + 1.0) for x in xs]
        logpmf = np.array(logC) + xs * np.log(max(1e-12, 1.0 - p)) + k * np.log(max(1e-12, p))
        pmf = np.exp(logpmf - np.max(logpmf))
        pmf = pmf / pmf.sum()
        return pmf
    ph = nb_pmf_vec(mu_h, k_h, max_goals)
    pa = nb_pmf_vec(mu_a, k_a, max_goals)
    P = np.outer(ph, pa)
    P = P / P.sum()
    return P

def _oneXtwo_from_matrix(P: np.ndarray) -> Dict[str, float]:
    home = float(np.tril(P, -1).sum())
    draw = float(np.diag(P).sum())
    away = float(np.triu(P, 1).sum())
    return {"p_H": home, "p_D": draw, "p_A": away}

# ---------------------------
# Calibration guards
# ---------------------------

def _model_predict_binary(model, p: np.ndarray) -> np.ndarray:
    try:
        from sklearn.isotonic import IsotonicRegression
        from sklearn.linear_model import LogisticRegression
    except Exception:
        IsotonicRegression = object  # type: ignore
        LogisticRegression = object  # type: ignore
    if hasattr(model, 'predict') and model.__class__.__name__ == 'IsotonicRegression':
        return np.clip(model.predict(p), 1e-6, 1-1e-6)
    if hasattr(model, 'predict_proba') and model.__class__.__name__ == 'LogisticRegression':
        return model.predict_proba(p.reshape(-1,1))[:,1]
    # Fallback: identity
    return p


def _is_degenerate_calibrator_1x2(cal: Dict[str, object]) -> bool:
    """Return True if the 1X2 calibrator maps inputs to near-constants (degenerate)."""
    try:
        if not cal or not all(k in cal for k in ('H','D','A')):
            return True
        grid = np.linspace(0.01, 0.99, 25).astype(float)
        stds = []
        for k in ('H','D','A'):
            preds = _model_predict_binary(cal[k], grid.reshape(-1,1).ravel())
            stds.append(float(np.nanstd(preds)))
        # If all three vary less than threshold, consider degenerate
        return all(s < 1e-2 for s in stds)
    except Exception:
        return True


def _is_degenerate_calibrator_binary(model: object) -> bool:
    """Return True if a single binary calibrator (isotonic/platt) is near-constant."""
    try:
        grid = np.linspace(0.01, 0.99, 25).astype(float)
        preds = _model_predict_binary(model, grid.reshape(-1,1).ravel())
        return float(np.nanstd(preds)) < 1e-2
    except Exception:
        return True

# ---------------------------
# Market evaluation helpers
# ---------------------------

DEFAULT_OU_LINES = [1.5, 2.5, 3.5]
DEFAULT_INTERVALS = [(0, 3), (1, 3), (2, 4), (2, 5), (3, 5), (3, 6)]


def _goals_distribution(P: np.ndarray) -> np.ndarray:
    """Return distribution over total goals t by summing P[i,j] where i+j=t."""
    n = P.shape[0]  # assumes square matrix (0..max_goals)
    max_total = (n - 1) * 2
    dist = np.zeros(max_total + 1, dtype=float)
    for i in range(n):
        for j in range(n):
            dist[i + j] += P[i, j]
    # numerical safety
    s = dist.sum()
    if s > 0:
        dist = dist / s
    return dist


def _eval_1x2(P: np.ndarray) -> Dict[str, float]:
    x = _oneXtwo_from_matrix(P)
    return {"H": x["p_H"], "D": x["p_D"], "A": x["p_A"]}


def _eval_double_chance(p1x2: Dict[str, float]) -> Dict[str, float]:
    return {
        "1X": p1x2["H"] + p1x2["D"],
        "X2": p1x2["D"] + p1x2["A"],
        "12": p1x2["H"] + p1x2["A"],
    }


def _eval_over_under(P: np.ndarray, lines: list[float]) -> Dict[str, Dict[str, float]]:
    dist = _goals_distribution(P)
    out: Dict[str, Dict[str, float]] = {}
    for line in lines:
        try:
            base = int(float(line))  # for 2.5 -> 2
        except Exception:
            continue
        under = float(dist[: base + 1].sum())  # totals 0..base
        over = float(dist[base + 1 :].sum())   # totals base+1 .. max
        out[f"{float(line):.1f}"] = {"Under": under, "Over": over}
    return out


def _eval_intervals(P: np.ndarray, intervals: list[tuple[int, int]]) -> Dict[str, float]:
    dist = _goals_distribution(P)
    out: Dict[str, float] = {}
    max_t = len(dist) - 1
    for a, b in intervals:
        a2 = max(0, int(a))
        b2 = min(max_t, int(b))
        if a2 > b2:
            prob = 0.0
        else:
            prob = float(dist[a2 : b2 + 1].sum())
        out[f"{a2}-{b2}"] = prob
    return out


def _evaluate_all_markets(P: np.ndarray,
                          ou_lines: Optional[list[float]] = None,
                          intervals: Optional[list[tuple[int, int]]] = None) -> Dict[str, Any]:
    p1x2 = _eval_1x2(P)
    dc = _eval_double_chance(p1x2)
    ou = _eval_over_under(P, ou_lines or DEFAULT_OU_LINES)
    ints = _eval_intervals(P, intervals or DEFAULT_INTERVALS)
    return {"1X2": p1x2, "DC": dc, "OU": ou, "Intervals": ints}

def _fused_matrix(mu_h_xgb: float, mu_a_xgb: float,
                  mu_h_dc: Optional[float], mu_a_dc: Optional[float],
                  w_dc: float, max_goals: int = 10) -> np.ndarray:
    """Combine XGB and DC score matrices with weight w_dc. Fallback to pure XGB if DC is unavailable or w_dc==0."""
    if mu_h_dc is None or mu_a_dc is None or w_dc <= 0.0:
        return _score_matrix(mu_h_xgb, mu_a_xgb, max_goals=max_goals)
    P_dc = _score_matrix(mu_h_dc, mu_a_dc, max_goals=max_goals)
    P_xgb = _score_matrix(mu_h_xgb, mu_a_xgb, max_goals=max_goals)
    P = w_dc * P_dc + (1.0 - w_dc) * P_xgb
    P /= P.sum()
    return P
def _score_from_cfg(mu_h: float, mu_a: float, cfg: Dict[str, Any], league: str, max_goals: int) -> np.ndarray:
    od = (cfg.get('overdispersion') or {})
    method = str(od.get('method','poisson')).lower()
    if method == 'negbin':
        k = float(od.get('k', 6.0))
        return _score_matrix_negbin(mu_h, mu_a, k_h=k, k_a=k, max_goals=max_goals)
    return _score_matrix(mu_h, mu_a, max_goals=max_goals, trim_epsilon=float(cfg.get('tail_epsilon', 0.0)))

def _feature_row_from_snapshot(stats: pd.DataFrame, home: str, away: str) -> Optional[pd.DataFrame]:
    """Build feature row in the order config.ULTIMATE_FEATURES expects using a team snapshot."""
    hs = stats.loc[stats["team"] == home]
    as_ = stats.loc[stats["team"] == away]
    if hs.empty or as_.empty:
        # Fallback: synthesize baseline rows using league medians so we don't drop fixtures
        try:
            med = stats.median(numeric_only=True)
            def make_side(prefix: str) -> pd.DataFrame:
                row = {
                    'xg_home_EWMA': float(med.get('xg_home_EWMA', med.get('xg_L5', 1.4))),
                    'xga_home_EWMA': float(med.get('xga_home_EWMA', med.get('xga_L5', 1.2))),
                    'xg_away_EWMA': float(med.get('xg_away_EWMA', med.get('xg_L5', 1.2))),
                    'xga_away_EWMA': float(med.get('xga_away_EWMA', med.get('xga_L5', 1.2))),
                    'ppg_home_EWMA': float(med.get('ppg_home_EWMA', med.get('gpg_L10', 1.5))),
                    'ppg_away_EWMA': float(med.get('ppg_away_EWMA', med.get('gpg_L10', 1.3))),
                    'corners_L10': float(med.get('corners_L10', 0.0)),
                    'corners_allowed_L10': float(med.get('corners_allowed_L10', 0.0)),
                    'elo': float(stats['elo'].mean()) if 'elo' in stats.columns else 1500.0,
                    # Optional band/sim features
                    'GFvsMid_H': float(med.get('GFvsMid_H', med.get('xg_L5', 1.3))),
                    'GAvsMid_H': float(med.get('GAvsMid_H', med.get('xga_L5', 1.2))),
                    'GFvsHigh_H': float(med.get('GFvsHigh_H', med.get('xg_L5', 1.1))),
                    'GAvsHigh_H': float(med.get('GAvsHigh_H', med.get('xga_L5', 1.3))),
                    'GFvsMid_A': float(med.get('GFvsMid_A', med.get('xg_L5', 1.1))),
                    'GAvsMid_A': float(med.get('GAvsMid_A', med.get('xga_L5', 1.2))),
                    'GFvsHigh_A': float(med.get('GFvsHigh_A', med.get('xg_L5', 1.0))),
                    'GAvsHigh_A': float(med.get('GAvsHigh_A', med.get('xga_L5', 1.4))),
                }
                return pd.DataFrame([row])
            if hs.empty:
                hs = make_side('H')
            if as_.empty:
                as_ = make_side('A')
        except Exception:
            return None
    feat_vals = []
    mapping = {
        # Prefer EWMA home/away splits; fallback to prior columns
        'ShotConv_H':            'xg_home_EWMA',
        'ShotConv_A':            'xg_away_EWMA',
        'ShotConvRec_H':         'xga_home_EWMA',
        'ShotConvRec_A':         'xga_away_EWMA',
        'PointsPerGame_H':       'ppg_home_EWMA',
        'PointsPerGame_A':       'ppg_away_EWMA',
        'CleanSheetStreak_H':    None,
        'CleanSheetStreak_A':    None,
        'xGDiff_H':              None,  # compute from ShotConv - ShotConvRec
        'xGDiff_A':              None,
        'CornersConv_H':         'corners_L10',
        'CornersConv_A':         'corners_L10',
        'CornersConvRec_H':      'corners_allowed_L10',
        'CornersConvRec_A':      'corners_allowed_L10',
        'NumMatches_H':          None,
        'NumMatches_A':          None,
        'Elo_H':                 'elo',
        'Elo_A':                 'elo',
        'EloDiff':               None,
        # Elo-band derived features
        'GFvsMid_H':             'GFvsMid_H',
        'GAvsMid_H':             'GAvsMid_H',
        'GFvsHigh_H':            'GFvsHigh_H',
        'GAvsHigh_H':            'GAvsHigh_H',
        'GFvsMid_A':             'GFvsMid_A',
        'GAvsMid_A':             'GAvsMid_A',
        'GFvsHigh_A':            'GFvsHigh_A',
        'GAvsHigh_A':            'GAvsHigh_A',
        # New: possession, corners totals, xG per possession, xG from corners
        'Possession_H':          'possession_home_EWMA',
        'Possession_A':          'possession_away_EWMA',
        'PossessionRec_H':       'possession_against_home_EWMA',
        'PossessionRec_A':       'possession_against_away_EWMA',
        'CornersFor_H':          'corners_for_home_EWMA',
        'CornersFor_A':          'corners_for_away_EWMA',
        'CornersAgainst_H':      'corners_against_home_EWMA',
        'CornersAgainst_A':      'corners_against_away_EWMA',
        'xGpp_H':                'xg_pp_home_EWMA',
        'xGpp_A':                'xg_pp_away_EWMA',
        'xGppRec_H':             'xg_pp_against_home_EWMA',
        'xGppRec_A':             'xg_pp_against_away_EWMA',
        'xGCorners_H':           'xg_from_corners_home_EWMA',
        'xGCorners_A':           'xg_from_corners_away_EWMA',
        'xGCornersRec_H':        'xg_from_corners_against_home_EWMA',
        'xGCornersRec_A':        'xg_from_corners_against_away_EWMA',
    }
    for col in config.ULTIMATE_FEATURES:
        key = mapping.get(col)
        if col == 'AvailabilityDiff':
            eh = float(hs.iloc[0].get('availability_index', 1.0))
            ea = float(as_.iloc[0].get('availability_index', 1.0))
            feat_vals.append(eh - ea)
            continue
        if col == 'Availability_H':
            feat_vals.append(float(hs.iloc[0].get('availability_index', 1.0)))
            continue
        if col == 'Availability_A':
            feat_vals.append(float(as_.iloc[0].get('availability_index', 1.0)))
            continue
        if col == 'xGDiff_H':
            val = (float(hs.iloc[0].get('xg_home_EWMA', np.nan)) - float(hs.iloc[0].get('xga_home_EWMA', np.nan)))
            feat_vals.append(val)
        elif col == 'xGDiff_A':
            val = (float(as_.iloc[0].get('xg_away_EWMA', np.nan)) - float(as_.iloc[0].get('xga_away_EWMA', np.nan)))
            feat_vals.append(val)
        elif col == 'NumMatches_H':
            feat_vals.append(20.0)
        elif col == 'NumMatches_A':
            feat_vals.append(20.0)
        elif col == 'EloDiff':
            eh = float(hs.iloc[0].get('elo', np.nan))
            ea = float(as_.iloc[0].get('elo', np.nan))
            feat_vals.append(eh - ea)
        elif col.endswith("_H"):
            if key and key in hs.columns:
                feat_vals.append(float(hs.iloc[0][key]))
            else:
                # Fallback legacy columns
                legacy = {
                    'ShotConv_H': 'xg_L5', 'ShotConvRec_H': 'xga_L5', 'PointsPerGame_H': 'gpg_L10',
                    'CornersConv_H': 'corners_L10', 'CornersConvRec_H': 'corners_allowed_L10', 'Elo_H': 'elo',
                    'GFvsMid_H': 'xg_L5', 'GAvsMid_H': 'xga_L5', 'GFvsHigh_H': 'xg_L5', 'GAvsHigh_H': 'xga_L5'
                }
                lk = legacy.get(col)
                feat_vals.append(float(hs.iloc[0].get(lk, 0.0)))
        elif col.endswith("_A"):
            if key and key in as_.columns:
                feat_vals.append(float(as_.iloc[0][key]))
            else:
                legacy = {
                    'ShotConv_A': 'xg_L5', 'ShotConvRec_A': 'xga_L5', 'PointsPerGame_A': 'gpg_L10',
                    'CornersConv_A': 'corners_L10', 'CornersConvRec_A': 'corners_allowed_L10', 'Elo_A': 'elo',
                    'GFvsMid_A': 'xg_L5', 'GAvsMid_A': 'xga_L5', 'GFvsHigh_A': 'xg_L5', 'GAvsHigh_A': 'xga_L5'
                }
                lk = legacy.get(col)
                feat_vals.append(float(as_.iloc[0].get(lk, 0.0)))
        else:
            # Unexpected columns
            feat_vals.append(0.0)
    return pd.DataFrame([feat_vals], columns=config.ULTIMATE_FEATURES)


def _read_any_fixture_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    cols = {c.strip().lower(): c for c in df.columns}
    def pick(aliases):
        for a in aliases:
            k = a.strip().lower()
            if k in cols:
                return cols[k]
        return None
    date_col = pick(["date", "matchdate", "kickoff", "ko"])
    home_col = pick(["home", "hometeam", "home_team", "home team", "home_team_api"])
    away_col = pick(["away", "awayteam", "away_team", "away team", "away_team_api"])
    if not (date_col and home_col and away_col):
        raise ValueError(f"Fixtures CSV {path} must contain date/home/away columns. Found: {list(df.columns)}")
    return pd.DataFrame({
        "date": pd.to_datetime(df[date_col], errors='coerce').dt.strftime("%Y-%m-%d %H:%M:%S"),
        "home_team_api": df[home_col].astype(str).str.strip(),
        "away_team_api": df[away_col].astype(str).str.strip(),
    })


def _fixtures_with_fallbacks(league_code: str, preferred_csv: Optional[str] = None) -> tuple[pd.DataFrame, str]:
    tried = []
    # 1) Explicit CSV provided
    if preferred_csv and os.path.exists(preferred_csv):
        try:
            return _read_any_fixture_csv(preferred_csv), f"from CSV: {preferred_csv}"
        except Exception:
            tried.append(preferred_csv)
    # 2) Weekly fixtures CSV
    weekly = os.path.join("data", "fixtures", f"{league_code}_weekly_fixtures.csv")
    if os.path.exists(weekly):
        try:
            return _read_any_fixture_csv(weekly), f"from CSV: {weekly}"
        except Exception:
            tried.append(weekly)
    # 3) Local odds JSON -> derive fixtures
    try:
        odds_path = os.path.join("data", "odds", f"{league_code}.json")
        if os.path.exists(odds_path):
            with open(odds_path, "r", encoding="utf-8") as f:
                data = json.load(f) or {}
            fx_list = data.get("fixtures") or data.get("events") or []
            recs = []
            for fx in fx_list:
                h = str(fx.get("home", "")).strip()
                a = str(fx.get("away", "")).strip()
                if not h or not a:
                    continue
                d = str(fx.get("date", "")).strip()
                # normalise to common string format if possible
                try:
                    dnorm = pd.to_datetime(d, errors='coerce')
                    dstr = dnorm.strftime("%Y-%m-%d %H:%M:%S") if pd.notna(dnorm) else None
                except Exception:
                    dstr = None
                recs.append({"date": dstr, "home_team_api": h, "away_team_api": a})
            if recs:
                return pd.DataFrame(recs), f"from odds JSON: {odds_path}"
    except Exception:
        pass
    # 4) Manual CSV (user-editable). Create header only if missing.
    manual = os.path.join("data", "fixtures", f"{league_code}_manual.csv")
    if os.path.exists(manual):
        try:
            return _read_any_fixture_csv(manual), f"from CSV: {manual}"
        except Exception as e:
            print(f"[fallback] Failed to read manual fixtures {manual}: {e}")
    else:
        os.makedirs(os.path.dirname(manual), exist_ok=True)
        with open(manual, "w", encoding="utf-8") as f:
            f.write("date,home,away\n")
    print(f"[fixtures] No weekly fixtures found. Using/created manual template: {manual}")
    return pd.DataFrame(columns=["date","home_team_api","away_team_api"]), "manual"

# --- Fusion engine ---
def generate_predictions(cfg: Dict[str, Any]) -> pd.DataFrame:
    """Return fused 1X2 probabilities for fixtures listed in data/fixtures.

    Expects models in advanced_models and enhanced features CSV present.
    Returns columns: date (if available), home, away, p_H, p_D, p_A
    """
    league = cfg.get('league', 'E0')
    max_goals = int(cfg.get('max_goals', config.MAX_GOALS_PER_LEAGUE.get(league, 10)))

    # Models (XGB only; DC removed)
    home_path = os.path.join("advanced_models", f"{league}_ultimate_xgb_home.pkl")
    away_path = os.path.join("advanced_models", f"{league}_ultimate_xgb_away.pkl")
    xgb_home = joblib.load(home_path)
    xgb_away = joblib.load(away_path)

    # Team snapshot (built from enhanced final features)
    enh_path = os.path.join("data", "enhanced", f"{league}_final_features.csv")
    snap = feature_store.build_snapshot(
        enhanced_csv=enh_path,
        as_of=None,
        half_life_matches=int(cfg.get('half_life_matches', 5)),
        elo_k=float(cfg.get('elo_k', 20.0)),
        elo_home_adv=float(cfg.get('elo_home_adv', 60.0)),
        micro_agg_path=(cfg.get('micro_agg_path') or os.path.join('data','enhanced', f'{league}_micro_agg.csv'))
    )
    # Build per-team Elo-similarity histories from processed data (for dynamic features)
    try:
        proc_path = os.path.join('data','processed', f'{league}_merged_preprocessed.csv')
        df_pro = pd.read_csv(proc_path)
        d_elos = xgb_trainer._compute_ewma_elo_prematch(
            df_pro, half_life_matches=int(cfg.get('half_life_matches', 5)),
            elo_k=float(cfg.get('elo_k', 20.0)), elo_home_adv=float(cfg.get('elo_home_adv', 60.0)),
            elo_similarity_sigma=float(cfg.get('elo_similarity_sigma', config.ELO_SIM_SIGMA_PER_LEAGUE.get(league, 50.0)))
        )
        hist_home: dict[str, list[tuple[float,float,float]]] = {}
        hist_away: dict[str, list[tuple[float,float,float]]] = {}
        for _, rr in d_elos.iterrows():
            try:
                home_t = str(rr['HomeTeam']); away_t = str(rr['AwayTeam'])
                eh = float(rr['Elo_H']); ea = float(rr['Elo_A'])
                fthg = float(rr.get('FTHG', pd.NA)) if 'FTHG' in rr else None
                ftag = float(rr.get('FTAG', pd.NA)) if 'FTAG' in rr else None
                if fthg is None or ftag is None or pd.isna(fthg) or pd.isna(ftag):
                    continue
                hist_home.setdefault(home_t, []).append((ea, fthg, ftag))
                hist_away.setdefault(away_t, []).append((eh, ftag, fthg))
            except Exception:
                continue
    except Exception:
        hist_home = {}
        hist_away = {}

    # Fixtures with fallbacks
    fixtures, source = _fixtures_with_fallbacks(league_code=league, preferred_csv=cfg.get('fixtures_csv'))

    out = []
    for _, row in fixtures.iterrows():
        home_api = str(row.get("home_team", row.get("home_team_api", ""))).strip()
        away_api = str(row.get("away_team", row.get("away_team_api", ""))).strip()
        date_val = row.get("date")
        # Normalize to dataset names
        home = config.normalize_team_name(home_api)
        away = config.normalize_team_name(away_api)
        try:
            hid = team_registry.get_team_id(league, home)
            aid = team_registry.get_team_id(league, away)
        except Exception:
            hid = None
            aid = None
        # Canonical IDs if available
        try:
            hid = team_registry.get_team_id(league, home)
            aid = team_registry.get_team_id(league, away)
        except Exception:
            hid = None
            aid = None
        try:
            hid = team_registry.get_team_id(league, home)
            aid = team_registry.get_team_id(league, away)
        except Exception:
            hid = None
            aid = None
        try:
            hid = team_registry.get_team_id(league, home)
            aid = team_registry.get_team_id(league, away)
        except Exception:
            hid = None
            aid = None
        try:
            hid = team_registry.get_team_id(league, home)
            aid = team_registry.get_team_id(league, away)
        except Exception:
            hid = None; aid = None

        feat_row = _feature_row_from_snapshot(snap, home, away)
        if feat_row is None:
            continue
        # Inject Elo-similarity dynamic features if history available
        try:
            import math
            sigma = float(cfg.get('elo_similarity_sigma', config.ELO_SIM_SIGMA_PER_LEAGUE.get(league, 50.0)))
            def kernel_mean(hist, center):
                if not hist:
                    return (0.0, 0.0)
                inv2s2 = 1.0/(2.0*max(sigma,1e-6)*max(sigma,1e-6))
                wsum=0.0; gf_sum=0.0; ga_sum=0.0
                for opp_elo, gf, ga in hist:
                    w = math.exp(- (opp_elo-center)**2 * inv2s2)
                    wsum += w; gf_sum += w*float(gf); ga_sum += w*float(ga)
                if wsum<=0: return (0.0,0.0)
                return (gf_sum/wsum, ga_sum/wsum)
            # current pre-match Elo from features
            eh_cur = float(feat_row['Elo_H'].iloc[0]) if 'Elo_H' in feat_row.columns else None
            ea_cur = float(feat_row['Elo_A'].iloc[0]) if 'Elo_A' in feat_row.columns else None
            if ea_cur is not None:
                gf_h, ga_h = kernel_mean(hist_home.get(home), ea_cur)
                feat_row['GFvsSim_H'] = [gf_h]
                feat_row['GAvsSim_H'] = [ga_h]
            if eh_cur is not None:
                gf_a, ga_a = kernel_mean(hist_away.get(away), eh_cur)
                feat_row['GFvsSim_A'] = [gf_a]
                feat_row['GAvsSim_A'] = [ga_a]
        except Exception:
            # fallback zeros if something goes wrong
            for k in ('GFvsSim_H','GAvsSim_H','GFvsSim_A','GAvsSim_A'):
                if k not in feat_row.columns:
                    feat_row[k] = [0.0]
        mu_h_xgb = _predict_xgb(xgb_home, feat_row)
        mu_a_xgb = _predict_xgb(xgb_away, feat_row)
        # Optional per-league priors: blend XGB means with league baselines
        priors = (cfg.get('league_priors') or {}).get(league)
        if priors:
            try:
                w = float(priors.get('weight', 0.0))
                mh = priors.get('mu_home', None)
                ma = priors.get('mu_away', None)
                if w > 0.0 and mh is not None and ma is not None:
                    mu_h_xgb = (1.0 - w) * mu_h_xgb + w * float(mh)
                    mu_a_xgb = (1.0 - w) * mu_a_xgb + w * float(ma)
            except Exception:
                pass
        # Optionally switch/blend with ShotXG aggregates (MicroXG)
        mu_h_final, mu_a_final, _src = _blend_mu(cfg, home, away, mu_h_xgb, mu_a_xgb)
        P = _score_from_cfg(mu_h_final, mu_a_final, cfg, league, max_goals)

        probs = _oneXtwo_from_matrix(P)
        out.append({"date": date_val, "home": home, "away": away, **probs})

    return pd.DataFrame(out)


def generate_market_book(cfg: Dict[str, Any]) -> pd.DataFrame:
    """Return a flattened market book with probabilities for multiple markets per match.

    Columns: date, home, away, market, outcome, prob
    Markets include 1X2, Double Chance, Over/Under (configurable lines), and total-goal intervals.
    """
    league = cfg.get('league', 'E0')
    use_calibration = bool(cfg.get('use_calibration', True))
    max_goals = int(cfg.get('max_goals', config.MAX_GOALS_PER_LEAGUE.get(league, 10)))

    # Parse market config
    ou_lines = cfg.get('ou_lines', DEFAULT_OU_LINES)
    try:
        ou_lines = [float(x) for x in ou_lines]
    except Exception:
        ou_lines = DEFAULT_OU_LINES
    intervals_cfg = cfg.get('intervals', DEFAULT_INTERVALS)
    intervals: list[tuple[int, int]] = []
    for iv in intervals_cfg:
        try:
            if isinstance(iv, (list, tuple)) and len(iv) == 2:
                intervals.append((int(iv[0]), int(iv[1])))
        except Exception:
            continue
    if not intervals:
        intervals = DEFAULT_INTERVALS

    # Models (XGB only; prefer JSON to avoid pickle warnings)
    home_json = os.path.join("advanced_models", f"{league}_ultimate_xgb_home.json")
    away_json = os.path.join("advanced_models", f"{league}_ultimate_xgb_away.json")
    home_pkl = os.path.join("advanced_models", f"{league}_ultimate_xgb_home.pkl")
    away_pkl = os.path.join("advanced_models", f"{league}_ultimate_xgb_away.pkl")
    def _load_model(json_path, pkl_path):
        try:
            if os.path.exists(json_path):
                m = XGBRegressor()
                m.load_model(json_path)
                return m
        except Exception:
            pass
        return joblib.load(pkl_path)
    xgb_home = _load_model(home_json, home_pkl)
    xgb_away = _load_model(away_json, away_pkl)

    enh_path = os.path.join("data", "enhanced", f"{league}_final_features.csv")
    snap = feature_store.build_snapshot(
        enhanced_csv=enh_path,
        as_of=None,
        half_life_matches=int(cfg.get('half_life_matches', 5)),
        elo_k=float(cfg.get('elo_k', 20.0)),
        elo_home_adv=float(cfg.get('elo_home_adv', 60.0)),
        micro_agg_path=(cfg.get('micro_agg_path') or os.path.join('data','enhanced', f'{league}_micro_agg.csv'))
    )
    fixtures, source = _fixtures_with_fallbacks(league_code=league)

    rows = []
    for _, r in fixtures.iterrows():
        home_api = str(r.get("home_team", r.get("home_team_api", ""))).strip()
        away_api = str(r.get("away_team", r.get("away_team_api", ""))).strip()
        date_val = r.get("date")
        home = config.normalize_team_name(home_api)
        away = config.normalize_team_name(away_api)

        feat_row = _feature_row_from_snapshot(snap, home, away)
        if feat_row is None:
            continue
        mu_h, mu_a, _ = _predict_goals_mu(cfg, league, feat_row, xgb_home, xgb_away)
        # Blend with MicroXG if configured
        mu_h_final, mu_a_final, _src = _blend_mu(cfg, home, away, mu_h, mu_a)
        P = _score_from_cfg(mu_h_final, mu_a_final, cfg, league, max_goals)

        markets = _evaluate_all_markets(P, ou_lines=ou_lines, intervals=intervals)
        # Apply calibration if available
        try:
            from calibrators import load_calibrators, apply_calibration_1x2
            cal1_path = os.path.join('calibrators', f"{league}_1x2.pkl")
            cal1 = load_calibrators(cal1_path)
        except Exception:
            cal1 = None
        if use_calibration and cal1 and not _is_degenerate_calibrator_1x2(cal1):
            arr = np.array([[markets['1X2']['H'], markets['1X2']['D'], markets['1X2']['A']]])
            arr_cal = apply_calibration_1x2(arr, cal1)[0]
            markets['1X2'] = {'H': float(arr_cal[0]), 'D': float(arr_cal[1]), 'A': float(arr_cal[2])}
        elif cal1:
            # Skip degenerate calibrator
            pass
        # Optional shrinkage toward neutral for stability
        try:
            shrink_1x2 = float(cfg.get('calibration_shrink', {}).get('oneX2', 0.0))
        except Exception:
            shrink_1x2 = 0.0
        if shrink_1x2 > 0:
            prior = 1.0/3.0
            markets['1X2'] = {
                'H': (1.0 - shrink_1x2) * markets['1X2']['H'] + shrink_1x2 * prior,
                'D': (1.0 - shrink_1x2) * markets['1X2']['D'] + shrink_1x2 * prior,
                'A': (1.0 - shrink_1x2) * markets['1X2']['A'] + shrink_1x2 * prior,
            }
        try:
            calou_path = os.path.join('calibrators', f"{league}_ou25.pkl")
            calou = load_calibrators(calou_path)
        except Exception:
            calou = None
        if use_calibration and calou and 'ou25' in calou and '2.5' in markets['OU'] and not _is_degenerate_calibrator_binary(calou.get('ou25')):
            model = calou['ou25']
            from sklearn.isotonic import IsotonicRegression
            from sklearn.linear_model import LogisticRegression
            over_p = np.array([markets['OU']['2.5']['Over']])
            if isinstance(model, IsotonicRegression):
                cal_over = float(np.clip(model.predict(over_p), 1e-6, 1-1e-6))
            elif isinstance(model, LogisticRegression):
                cal_over = float(model.predict_proba(over_p.reshape(-1,1))[:,1][0])
            else:
                cal_over = float(over_p[0])
            markets['OU']['2.5'] = {'Over': cal_over, 'Under': float(max(0.0, 1.0 - cal_over))}
        elif calou:
            # Skip degenerate OU calibrator
            pass
        # Optional shrinkage toward 50/50 for OU 2.5
        try:
            shrink_ou = float(cfg.get('calibration_shrink', {}).get('ou25', 0.0))
        except Exception:
            shrink_ou = 0.0
        if shrink_ou > 0 and '2.5' in markets['OU']:
            pO = markets['OU']['2.5']['Over']
            pU = markets['OU']['2.5']['Under']
            pO = (1.0 - shrink_ou) * pO + shrink_ou * 0.5
            pU = 1.0 - pO
            markets['OU']['2.5'] = {'Over': float(pO), 'Under': float(pU)}
        # TG Interval calibration (optional)
        cal_tg = None
        try:
            from calibrators import load_calibrators
            cal_tg = load_calibrators(os.path.join('calibrators', f"{league}_tg.pkl"))
        except Exception:
            cal_tg = None
        if cal_tg and isinstance(markets.get('Intervals'), dict):
            ints_cal = {}
            for iv, pv in markets['Intervals'].items():
                model = cal_tg.get(iv)
                if model is None:
                    ints_cal[iv] = pv
                    continue
                try:
                    from sklearn.isotonic import IsotonicRegression
                    from sklearn.linear_model import LogisticRegression
                    x = np.array([pv], dtype=float)
                    if isinstance(model, IsotonicRegression):
                        y = float(np.clip(model.predict(x)[0], 1e-6, 1-1e-6))
                    elif isinstance(model, LogisticRegression):
                        y = float(model.predict_proba(x.reshape(-1,1))[0,1])
                    else:
                        y = pv
                except Exception:
                    y = pv
                ints_cal[iv] = y
            markets['Intervals'] = ints_cal

        # Flatten to rows
        # 1X2
        for k, v in markets["1X2"].items():
            rows.append({"date": date_val, "home": home, "away": away, "market": "1X2", "outcome": k, "prob": v})
        # Double Chance
        for k, v in markets["DC"].items():
            rows.append({"date": date_val, "home": home, "away": away, "market": "DC", "outcome": k, "prob": v})
        # Over/Under
        for line, kv in markets["OU"].items():
            rows.append({"date": date_val, "home": home, "away": away, "market": f"OU {line}", "outcome": "Under", "prob": kv["Under"]})
            rows.append({"date": date_val, "home": home, "away": away, "market": f"OU {line}", "outcome": "Over", "prob": kv["Over"]})
        # Intervals
        for iv, pv in markets["Intervals"].items():
            rows.append({"date": date_val, "home": home, "away": away, "market": "TG Interval", "outcome": iv, "prob": pv})

    return pd.DataFrame(rows)


# ---------------------------
# Phase 3: Odds, margin removal, Edge/EV
# ---------------------------

def _placeholder_odds(market: str, outcome: str) -> float:
    """Return a simple placeholder decimal odd for offline testing.

    These are not meant to be realistic; they enable EV/edge computation offline.
    """
    if market == "1X2":
        return 2.0
    if market == "DC":
        return 1.33
    if market.startswith("OU "):
        return 1.90
    if market == "TG Interval":
        return 3.00
    return 2.0


def _normalize_overround(group: pd.DataFrame) -> pd.DataFrame:
    """Given a market group with 'odds', compute normalized implied probabilities.

    Adds columns: p_imp, p_imp_norm
    """
    g = group.copy()
    g["p_imp"] = 1.0 / g["odds"].astype(float)
    s = g["p_imp"].sum()
    if s > 0:
        g["p_imp_norm"] = g["p_imp"] / s
    else:
        g["p_imp_norm"] = g["p_imp"]
    return g


def _market_is_exclusive(market: str) -> bool:
    """Return True if outcomes are mutually exclusive and cover the market.

    We treat '1X2' and 'OU <line>' as exclusive; 'DC' and 'TG Interval' are overlapping, so we do not normalize them.
    """
    m = str(market)
    return (m == '1X2') or m.startswith('OU ')


def attach_value_metrics(market_df: pd.DataFrame, use_placeholders: bool = True) -> pd.DataFrame:
    """Attach odds, normalized implied probability, edge and EV to market probabilities.

    Expects columns: date, home, away, market, outcome, prob
    Returns a new DataFrame with added columns: odds, p_imp, p_imp_norm, edge, EV
    """
    if market_df.empty:
        return market_df.copy()
    df = market_df.copy()
    if use_placeholders:
        df["odds"] = [
            _placeholder_odds(m, o) for m, o in zip(df["market"].astype(str), df["outcome"].astype(str))
        ]
    # Group by market per match for normalization
    grouped = []
    for (date, home, away, market), sub in df.groupby(["date", "home", "away", "market"], dropna=False):
        sub2 = sub.copy()
        # implied prob from odds
        sub2["p_imp"] = 1.0 / sub2["odds"].astype(float)
        if _market_is_exclusive(market):
            s = sub2["p_imp"].sum()
            sub2["p_imp_norm"] = sub2["p_imp"] / s if s > 0 else sub2["p_imp"]
        else:
            # For overlapping outcomes, keep p_imp_norm equal to p_imp (no overround normalization)
            sub2["p_imp_norm"] = sub2["p_imp"]
        # Edge and EV
        sub2["edge"] = sub2["prob"] - sub2["p_imp_norm"]
        sub2["EV"] = sub2["prob"] * sub2["odds"] - 1.0
        # If opening/closing odds are present, compute deltas of implied prob
        if "odds_open" in sub2.columns and "odds_close" in sub2.columns:
            try:
                sub2["p_imp_open"] = 1.0 / sub2["odds_open"].astype(float)
                sub2["p_imp_close"] = 1.0 / sub2["odds_close"].astype(float)
                if _market_is_exclusive(str(market)):
                    so = sub2["p_imp_open"].sum(); sc = sub2["p_imp_close"].sum()
                    if so > 0:
                        sub2["p_imp_open_norm"] = sub2["p_imp_open"] / so
                    if sc > 0:
                        sub2["p_imp_close_norm"] = sub2["p_imp_close"] / sc
                else:
                    sub2["p_imp_open_norm"] = sub2["p_imp_open"]
                    sub2["p_imp_close_norm"] = sub2["p_imp_close"]
                if "p_imp_open_norm" in sub2.columns and "p_imp_close_norm" in sub2.columns:
                    sub2["delta_p_imp"] = sub2["p_imp_close_norm"] - sub2["p_imp_open_norm"]
            except Exception:
                pass
        grouped.append(sub2)
    out = pd.concat(grouped, ignore_index=True) if grouped else df
    return out


# --- Thresholds, odds filling, and CLI ---
def _market_base(m: str) -> str:
    return m.split()[0] if m.startswith("OU ") else m


def _get_thresholds(cfg: Dict[str, Any]) -> Dict[str, Dict[str, float]]:
    defaults = {
        '1X2': {'min_prob': 0.55, 'min_edge': 0.03},
        'DC': {'min_prob': 0.75, 'min_edge': 0.02},
        'OU': {'min_prob': 0.58, 'min_edge': 0.02},
        'TG Interval': {'min_prob': 0.30, 'min_edge': 0.05},
    }
    user = cfg.get('thresholds', {}) or {}
    for k, v in user.items():
        if k in defaults and isinstance(v, dict):
            for kk in ('min_prob','min_edge'):
                if kk in v:
                    defaults[k][kk] = float(v[kk])
    return defaults

def compute_tg_ci(cfg: Dict[str, Any], level: float = 0.8) -> Dict[tuple, str]:
    league = cfg.get('league','E0')
    max_goals = int(cfg.get('max_goals', config.MAX_GOALS_PER_LEAGUE.get(league, 10)))
    home_json = os.path.join("advanced_models", f"{league}_ultimate_xgb_home.json")
    away_json = os.path.join("advanced_models", f"{league}_ultimate_xgb_away.json")
    home_pkl = os.path.join("advanced_models", f"{league}_ultimate_xgb_home.pkl")
    away_pkl = os.path.join("advanced_models", f"{league}_ultimate_xgb_away.pkl")
    def _load_model(json_path, pkl_path):
        try:
            if os.path.exists(json_path):
                m = XGBRegressor()
                m.load_model(json_path)
                return m
        except Exception:
            pass
        return joblib.load(pkl_path)
    xgb_home = _load_model(home_json, home_pkl)
    xgb_away = _load_model(away_json, away_pkl)
    enh_path = os.path.join("data", "enhanced", f"{league}_final_features.csv")
    snap = feature_store.build_snapshot(
        enhanced_csv=enh_path,
        as_of=None,
        half_life_matches=int(cfg.get('half_life_matches', 5)),
        elo_k=float(cfg.get('elo_k', 20.0)),
        elo_home_adv=float(cfg.get('elo_home_adv', 60.0)),
        micro_agg_path=cfg.get('micro_agg_path', os.path.join('data','enhanced','micro_agg.csv'))
    )
    fixtures, _ = _fixtures_with_fallbacks(league_code=league, preferred_csv=cfg.get('fixtures_csv'))
    out = {}
    for _, r in fixtures.iterrows():
        home_api = str(r.get("home_team", r.get("home_team_api", ""))).strip()
        away_api = str(r.get("away_team", r.get("away_team_api", ""))).strip()
        date_val = r.get("date")
        home = config.normalize_team_name(home_api)
        away = config.normalize_team_name(away_api)
        feat_row = _feature_row_from_snapshot(snap, home, away)
        if feat_row is None:
            continue
        mu_h = _predict_xgb(xgb_home, feat_row)
        mu_a = _predict_xgb(xgb_away, feat_row)
        P = _score_from_cfg(mu_h, mu_a, cfg, league, max_goals)
        dist = _goals_distribution(P)
        n = len(dist)
        csum = np.cumsum(dist)
        best = (0, n-1, 1e9)
        for a in range(n):
            for b in range(a, n):
                mass = csum[b] - (csum[a-1] if a>0 else 0.0)
                if mass >= level:
                    if (b-a) < best[2]:
                        best = (a,b,b-a)
                    break
        a,b,_ = best
        out[(str(date_val), home, away)] = f"{a}-{b}"
    return out


def _fill_odds_for_df(df: pd.DataFrame, league: str, with_odds: bool) -> pd.DataFrame:
    if df.empty:
        return df
    out = df.copy()
    out['odds'] = None
    # Optional opening/closing odds for movement analysis
    out['odds_open'] = None
    out['odds_close'] = None
    for (date, home, away), sub_idx in out.groupby(['date','home','away']).groups.items():
        idx = list(sub_idx)
        # Pre-fetch 1X2 when --with-odds
        if with_odds:
            try:
                odds1x2 = bookmaker_api.get_odds(league, home, away)
            except Exception:
                odds1x2 = {}
        else:
            odds1x2 = {}
        for i in idx:
            m = str(out.at[i,'market'])
            o = str(out.at[i,'outcome'])
            if m == '1X2' and with_odds and odds1x2:
                if o == 'H' and odds1x2.get('home'):
                    out.at[i,'odds'] = float(odds1x2['home'])
                elif o == 'D' and odds1x2.get('draw'):
                    out.at[i,'odds'] = float(odds1x2['draw'])
                elif o == 'A' and odds1x2.get('away'):
                    out.at[i,'odds'] = float(odds1x2['away'])
                # opening/closing if provided
                op = odds1x2.get('_open') or {}
                cl = odds1x2.get('_close') or {}
                if o == 'H' and op.get('home'):
                    out.at[i,'odds_open'] = float(op['home'])
                if o == 'D' and op.get('draw'):
                    out.at[i,'odds_open'] = float(op['draw'])
                if o == 'A' and op.get('away'):
                    out.at[i,'odds_open'] = float(op['away'])
                if o == 'H' and cl.get('home'):
                    out.at[i,'odds_close'] = float(cl['home'])
                if o == 'D' and cl.get('draw'):
                    out.at[i,'odds_close'] = float(cl['draw'])
                if o == 'A' and cl.get('away'):
                    out.at[i,'odds_close'] = float(cl['away'])
            elif m.startswith('OU ') and with_odds:
                try:
                    line = float(m.split(' ',1)[1])
                    ou_odds = bookmaker_api.get_odds_ou(league, home, away, line)
                except Exception:
                    ou_odds = {}
                if o in ('Over','Under') and ou_odds.get(o):
                    out.at[i,'odds'] = float(ou_odds[o])
                    op = ou_odds.get('_open') or {}
                    cl = ou_odds.get('_close') or {}
                    if op.get(o):
                        out.at[i,'odds_open'] = float(op[o])
                    if cl.get(o):
                        out.at[i,'odds_close'] = float(cl[o])
            elif m == 'TG Interval' and with_odds:
                try:
                    a_str, b_str = o.split('-')
                    odd_val = bookmaker_api.get_odds_interval(league, home, away, int(a_str), int(b_str))
                    if odd_val:
                        out.at[i,'odds'] = float(odd_val)
                except Exception:
                    pass
            if out.at[i,'odds'] is None:
                out.at[i,'odds'] = _placeholder_odds(m, o)
    return out


def main() -> None:
    import argparse
    ap = argparse.ArgumentParser(description="Hybrid DC+xG market engine with value metrics")
    ap.add_argument('--with-odds', action='store_true', help='Fetch 1X2 odds from bookmaker API (network needed)')
    ap.add_argument('--top', type=int, default=20, help='Show top N picks by EV')
    args = ap.parse_args()

    cfg = load_config()
    print("\n--- Hybrid Engine Started ---")
    print(cfg)

    market_df = generate_market_book(cfg)
    if market_df.empty:
        print("No fixtures available.")
        return

    df_with_odds = _fill_odds_for_df(market_df, cfg.get('league','E0'), with_odds=args.with_odds)
    df_val = attach_value_metrics(df_with_odds, use_placeholders=False)
    # Filter out very low odds (e.g., < 1.60)
    try:
        min_odds = float(cfg.get('min_odds', 1.6))
    except Exception:
        min_odds = 1.6
    if 'odds' in df_val.columns:
        df_val = df_val[pd.to_numeric(df_val['odds'], errors='coerce') >= float(min_odds)].copy()

    thresholds = _get_thresholds(cfg)
    def pass_threshold(row):
        base = _market_base(str(row['market']))
        th = thresholds.get(base, thresholds.get('1X2'))
        return (float(row['prob']) >= th['min_prob']) and (float(row['edge']) >= th['min_edge'])

    picks = df_val[df_val.apply(pass_threshold, axis=1)].copy()
    if picks.empty:
        print("No value picks under current thresholds.")
        return

    picks.sort_values(['EV','prob'], ascending=[False, False], inplace=True)
    show = picks.head(args.top)

    # Export full ranked picks to CSV
    try:
        ts = pd.Timestamp.utcnow().strftime('%Y%m%dT%H%M%SZ')
        rep_dir = Path('reports'); rep_dir.mkdir(parents=True, exist_ok=True)
        out_csv = rep_dir / f"{cfg.get('league','E0')}_{ts}_picks.csv"
        cols = ['date','home','away','market','outcome','prob','odds','edge','EV']
        picks.to_csv(out_csv, index=False, columns=[c for c in cols if c in picks.columns])
        print(f"Saved picks CSV -> {out_csv}")
    except Exception as e:
        print(f"Could not save picks CSV: {e}")

    print("\n--- Top Picks (by EV) ---")
    for _, r in show.iterrows():
        print(f"{r['date']}  {r['home']} vs {r['away']} | {r['market']} {r['outcome']}  p={float(r['prob']):.2%}  odds={float(r['odds']):.2f}  edge={float(r['edge']):.2%}  EV={float(r['EV']):.2f}")

    # Portfolio risk control (correlation-aware throttling)
    def _compute_cluster_key(row) -> tuple:
        # cluster by date (day) + market base by default
        d = str(row.get('date'))[:10]
        m = _market_base(str(row.get('market','')))
        return (d, m)
    portfolio_cfg = cfg.get('portfolio', {}) or {}
    throttle = bool(portfolio_cfg.get('throttle', True))
    min_mult = float(portfolio_cfg.get('min_mult', 0.4))
    mode = str(portfolio_cfg.get('throttle_mode','sqrt')).lower()  # 'sqrt' or 'linear'
    # Precompute cluster sizes
    clusters = {}
    for _, r in show.iterrows():
        k = _compute_cluster_key(r)
        clusters[k] = clusters.get(k, 0) + 1

    # Log bets (optional)
    if cfg.get('log_bets', True):
        bm = BankrollManager()
        flat_stake = float(cfg.get('stake_size', 10.0))
        policy = cfg.get('stake_policy','flat')
        k_frac = float(cfg.get('kelly_fraction', 0.25))
        for _, r in show.iterrows():
            ev = float(r['EV'])
            if ev <= 0:
                continue
            p = float(r['prob']); O = float(r['odds'])
            if policy == 'kelly':
                # Kelly fraction per bet
                try:
                    f_star = (p*O - (1.0 - p)) / max(O - 1.0, 1e-9)
                except Exception:
                    f_star = 0.0
                f_star = max(0.0, min(1.0, f_star))
                stake = max(0.0, min(bm.bankroll, bm.bankroll * k_frac * f_star))
            else:
                stake = min(flat_stake, bm.bankroll)
            # Apply portfolio throttle multiplier based on cluster size
            if throttle:
                k = _compute_cluster_key(r)
                n = max(1, clusters.get(k, 1))
                if mode == 'linear':
                    mult = max(min_mult, 1.0 / float(n))
                else:
                    mult = max(min_mult, 1.0 / float(n)**0.5)
                stake *= mult
            if stake <= 0:
                continue
            bm.log_bet(
                date=str(r['date']),
                league=cfg.get('league','E0'),
                home=str(r['home']),
                away=str(r['away']),
                market=f"{r['market']} {r['outcome']}",
                odds=O,
                stake=stake,
                prob=p,
                ev=ev
            )
    else:
        print("Print-only mode: set log_bets=true to record stakes.")


# Note: keep __main__ at the end of file so that all helpers are defined before CLI runs
def _predict_xgb(model, feat_df: pd.DataFrame) -> float:
    """Predict with XGB model using model's known feature names if available to avoid name mismatch."""
    try:
        names = getattr(model, 'feature_names_in_', None)
        if names is None:
            try:
                names = model.get_booster().feature_names
            except Exception:
                names = None
        if names is not None:
            # ensure order and subset strictly to expected names
            cols = [c for c in names if c in feat_df.columns]
            X = feat_df[cols].values
        else:
            X = feat_df.values
        y = model.predict(X)
        return float(y[0] if hasattr(y, '__len__') else y)
    except Exception:
        # Fallback: try slicing first n features if model exposes n_features_in_
        try:
            n = int(getattr(model, 'n_features_in_', 0))
            if n > 0:
                X = feat_df.iloc[:, :n].values
                y = model.predict(X)
                return float(y[0] if hasattr(y, '__len__') else y)
        except Exception:
            pass
        y = model.predict(feat_df.values)
        return float(y[0] if hasattr(y, '__len__') else y)


# --- Probabilistic model integration (Phase 3) ---
_PROB_MODEL_CACHE: Dict[str, dict] = {}
_MICRO_AGG_CACHE: Dict[str, dict] = {}

def _load_prob_models(league: str) -> dict:
    if league in _PROB_MODEL_CACHE:
        return _PROB_MODEL_CACHE[league]
    models = {}
    # NGBoost Poisson models
    try:
        hp = Path('advanced_models') / f'{league}_ngb_poisson_home.pkl'
        ap = Path('advanced_models') / f'{league}_ngb_poisson_away.pkl'
        if hp.exists() and ap.exists():
            models['ngb_poisson_home'] = joblib.load(hp)
            models['ngb_poisson_away'] = joblib.load(ap)
    except Exception:
        pass
    _PROB_MODEL_CACHE[league] = models
    return models


def _load_micro_map(path: str) -> dict:
    """Load micro aggregates CSV and return mapping (team_std, side) -> xg_for_EWMA.

    Team names are normalized via config.normalize_team_name to match engine names.
    """
    if not path:
        return {}
    if path in _MICRO_AGG_CACHE:
        return _MICRO_AGG_CACHE[path]
    try:
        import pandas as pd  # local import
        if not os.path.exists(path):
            _MICRO_AGG_CACHE[path] = {}
            return _MICRO_AGG_CACHE[path]
        df = pd.read_csv(path)
        # Normalize
        if 'team' not in df.columns or 'side' not in df.columns:
            _MICRO_AGG_CACHE[path] = {}
            return _MICRO_AGG_CACHE[path]
        try:
            df['date'] = pd.to_datetime(df.get('date'), errors='coerce')
        except Exception:
            pass
        df['team'] = df['team'].astype(str).map(config.normalize_team_name)
        # Keep latest per (team, side)
        df = df.sort_values(['team','side','date'])
        last = df.groupby(['team','side'], as_index=False).tail(1)
        m = {}
        for _, r in last.iterrows():
            try:
                key = (str(r['team']), str(r['side']).upper()[:1])
                val = float(r.get('xg_for_EWMA', r.get('xg_for', 0.0)) or 0.0)
                if val <= 0:
                    # fallback to per-match xg_for if EWMA missing/nonpositive
                    val = float(r.get('xg_for', 0.0) or 0.0)
                m[key] = max(0.05, val)
            except Exception:
                continue
        _MICRO_AGG_CACHE[path] = m
        return m
    except Exception:
        _MICRO_AGG_CACHE[path] = {}
        return _MICRO_AGG_CACHE[path]


def _blend_mu(cfg: Dict[str, Any], home: str, away: str, mu_h_macro: float, mu_a_macro: float) -> tuple[float, float, str]:
    src = str(cfg.get('xg_source', 'macro')).lower()
    if src not in ('macro','micro','blend'):
        src = 'macro'
    micro_path = cfg.get('micro_agg_path', os.path.join('data','enhanced','micro_agg.csv'))
    micro = _load_micro_map(micro_path) if src in ('micro','blend') else {}
    mu_h_micro = micro.get((home, 'H'))
    mu_a_micro = micro.get((away, 'A'))
    if src == 'micro':
        if mu_h_micro is not None and mu_a_micro is not None:
            return float(mu_h_micro), float(mu_a_micro), 'micro'
        return mu_h_macro, mu_a_macro, 'macro'
    if src == 'blend':
        try:
            w = float(cfg.get('xg_blend_weight', 0.5))
        except Exception:
            w = 0.5
        if mu_h_micro is not None:
            mu_h_macro = (1.0 - w) * mu_h_macro + w * float(mu_h_micro)
        if mu_a_micro is not None:
            mu_a_macro = (1.0 - w) * mu_a_macro + w * float(mu_a_micro)
        return mu_h_macro, mu_a_macro, 'blend'
    return mu_h_macro, mu_a_macro, 'macro'

def _predict_goals_mu(cfg: Dict[str, Any], league: str, feat_row: pd.DataFrame,
                      xgb_home_model, xgb_away_model) -> tuple[float, float, str]:
    pm = (cfg.get('prob_model') or {})
    enabled = bool(pm.get('enabled', False))
    kind = str(pm.get('kind','')).lower()
    if enabled and kind in ('ngboost_poisson','ngb_poisson','ngb_pois'):
        models = _load_prob_models(league)
        mH = models.get('ngb_poisson_home'); mA = models.get('ngb_poisson_away')
        if mH is not None and mA is not None:
            try:
                # NGBoost predict returns expectation under Poisson dist by default via .predict
                mu_h = float(mH.predict(feat_row)[0])
                mu_a = float(mA.predict(feat_row)[0])
                # Guard against non-positive
                mu_h = max(0.05, mu_h); mu_a = max(0.05, mu_a)
                return mu_h, mu_a, 'ngb_poisson'
            except Exception:
                pass
    # Fallback to XGB
    return _predict_xgb(xgb_home_model, feat_row), _predict_xgb(xgb_away_model, feat_row), 'xgb'
 
if __name__ == "__main__":
    main()
